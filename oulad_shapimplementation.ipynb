{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score,balanced_accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anonymisedData/studentRegistration.csv',\n",
       " 'anonymisedData/studentVle.csv',\n",
       " 'anonymisedData/vle.csv',\n",
       " 'anonymisedData/assessments.csv',\n",
       " 'anonymisedData/studentInfo.csv',\n",
       " 'anonymisedData/courses.csv',\n",
       " 'anonymisedData/studentAssessment.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = glob.glob('anonymisedData/*.csv')\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=[]\n",
    "# for i in range(len(all_files)):\n",
    "#     name = \"df{}\".format(i)\n",
    "#     print(name)\n",
    "#     a.append(name)\n",
    "#     str(a[i])= pd.read_csv(all_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('anonymisedData/studentRegistration.csv')\n",
    "df1 = pd.read_csv('anonymisedData/studentVle.csv')\n",
    "df2 = pd.read_csv('anonymisedData/vle.csv')\n",
    "df3 = pd.read_csv('anonymisedData/assessments.csv')\n",
    "df4 = pd.read_csv('anonymisedData/studentInfo.csv')\n",
    "df5 = pd.read_csv('anonymisedData/courses.csv')\n",
    "df6 = pd.read_csv('anonymisedData/studentAssessment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code_module', 'code_presentation', 'id_student', 'date_registration',\n",
      "       'date_unregistration'],\n",
      "      dtype='object')\n",
      "Index(['code_module', 'code_presentation', 'id_student', 'id_site', 'date',\n",
      "       'sum_click'],\n",
      "      dtype='object')\n",
      "Index(['id_site', 'code_module', 'code_presentation', 'activity_type',\n",
      "       'week_from', 'week_to'],\n",
      "      dtype='object')\n",
      "Index(['code_module', 'code_presentation', 'id_assessment', 'assessment_type',\n",
      "       'date', 'weight'],\n",
      "      dtype='object')\n",
      "Index(['code_module', 'code_presentation', 'id_student', 'gender', 'region',\n",
      "       'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts',\n",
      "       'studied_credits', 'disability', 'final_result'],\n",
      "      dtype='object')\n",
      "Index(['code_module', 'code_presentation', 'module_presentation_length'], dtype='object')\n",
      "Index(['id_assessment', 'id_student', 'date_submitted', 'is_banked', 'score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df0.columns)\n",
    "print(df1.columns)\n",
    "print(df2.columns) \n",
    "print(df3.columns)\n",
    "print(df4.columns) \n",
    "print(df5.columns) \n",
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32593, 5) (10655280, 6) (6364, 6) (206, 6) (32593, 12) (22, 3) (173912, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df0.shape, df1.shape, df2.shape, df3.shape, df4.shape, df5.shape, df6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01 = pd.merge(df0, df1, on = ['id_student','code_module', 'code_presentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10655280, 8)\n",
      "Index(['code_module', 'code_presentation', 'id_student', 'date_registration',\n",
      "       'date_unregistration', 'id_site', 'date', 'sum_click'],\n",
      "      dtype='object')\n",
      "Index(['id_site', 'code_module', 'code_presentation', 'activity_type',\n",
      "       'week_from', 'week_to'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df01.shape)\n",
    "print(df01.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df012 = pd.merge(df01, df2, on = ['code_module', 'code_presentation','id_site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10655280, 11)\n",
      "Index(['code_module', 'code_presentation', 'id_student', 'date_registration',\n",
      "       'date_unregistration', 'id_site', 'date', 'sum_click', 'activity_type',\n",
      "       'week_from', 'week_to'],\n",
      "      dtype='object')\n",
      "Index(['code_module', 'code_presentation', 'id_assessment', 'assessment_type',\n",
      "       'date', 'weight'],\n",
      "      dtype='object')\n",
      "Index(['code_module', 'code_presentation', 'id_student', 'gender', 'region',\n",
      "       'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts',\n",
      "       'studied_credits', 'disability', 'final_result'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df012.shape)\n",
    "print(df012.columns)\n",
    "print(df3.columns)\n",
    "print(df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0124 = pd.merge(df012, df4, on = ['code_module', 'code_presentation', 'id_student'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10655280, 20)\n",
      "Index(['code_module', 'code_presentation', 'id_student', 'date_registration',\n",
      "       'date_unregistration', 'id_site', 'date', 'sum_click', 'activity_type',\n",
      "       'week_from', 'week_to', 'gender', 'region', 'highest_education',\n",
      "       'imd_band', 'age_band', 'num_of_prev_attempts', 'studied_credits',\n",
      "       'disability', 'final_result'],\n",
      "      dtype='object')\n",
      "Index(['code_module', 'code_presentation', 'id_assessment', 'assessment_type',\n",
      "       'date', 'weight'],\n",
      "      dtype='object')\n",
      "Index(['code_module', 'code_presentation', 'module_presentation_length'], dtype='object')\n",
      "Index(['id_assessment', 'id_student', 'date_submitted', 'is_banked', 'score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df0124.shape)\n",
    "print(df0124.columns)\n",
    "print(df3.columns)\n",
    "print(df5.columns)\n",
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df35 = pd.merge(df3, df5, on = ['code_module', 'code_presentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df356 = pd.merge(df35, df6, on = ['id_assessment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code_module', 'code_presentation', 'id_student', 'date_registration',\n",
      "       'date_unregistration', 'id_site', 'date', 'sum_click', 'activity_type',\n",
      "       'week_from', 'week_to', 'gender', 'region', 'highest_education',\n",
      "       'imd_band', 'age_band', 'num_of_prev_attempts', 'studied_credits',\n",
      "       'disability', 'final_result'],\n",
      "      dtype='object') (10655280, 20)\n",
      "Index(['code_module', 'code_presentation', 'id_assessment', 'assessment_type',\n",
      "       'date', 'weight', 'module_presentation_length', 'id_student',\n",
      "       'date_submitted', 'is_banked', 'score'],\n",
      "      dtype='object') (173912, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df0124.columns, df0124.shape)\n",
    "\n",
    "print(df356.columns, df356.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code_module', 'code_presentation', 'id_student', 'gender', 'region',\n",
      "       'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts',\n",
      "       'studied_credits', 'disability', 'final_result'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3564 = pd.merge(df356, df4, on = ['code_module', 'code_presentation', 'id_student'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3564.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                       0\n",
      "code_module                      0\n",
      "code_presentation                0\n",
      "id_assessment                    0\n",
      "assessment_type                  0\n",
      "date                          2865\n",
      "weight                           0\n",
      "module_presentation_length       0\n",
      "id_student                       0\n",
      "date_submitted                   0\n",
      "is_banked                        0\n",
      "score                          173\n",
      "gender                           0\n",
      "region                           0\n",
      "highest_education                0\n",
      "imd_band                      7697\n",
      "age_band                         0\n",
      "num_of_prev_attempts             0\n",
      "studied_credits                  0\n",
      "disability                       0\n",
      "final_result                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_df.apply(lambda x : sum(x.isnull())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop('imd_band', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173912, 20)\n"
     ]
    }
   ],
   "source": [
    "print(final_df.shape)\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170874, 20)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "list_encode = ['code_module', 'code_presentation', 'assessment_type', 'gender', 'region', 'highest_education', 'age_band', 'disability','final_result']\n",
    "# Encode labels in column 'species'.\n",
    "for name in list_encode:\n",
    "    final_df[name]= label_encoder.fit_transform(final_df[name]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(['final_result'], axis=1)\n",
    "y = final_df['final_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt, Xv, yt, yv = train_test_split(X,y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 83361, 1: 22466, 0: 20448, 3: 10424})"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(list(yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.06%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69      5272\n",
      "           1       0.67      0.59      0.63      5536\n",
      "           2       0.82      0.89      0.86     20768\n",
      "           3       0.62      0.35      0.44      2599\n",
      "\n",
      "    accuracy                           0.77     34175\n",
      "   macro avg       0.70      0.63      0.65     34175\n",
      "weighted avg       0.76      0.77      0.76     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7617872248442896"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb=meboost.KNeighborsClassifier()\n",
    "mb.fit(Xt, yt)\n",
    "y_pred = mb.predict(Xv)\n",
    "accuracy = accuracy_score(yv, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.01%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.12      0.19      5272\n",
      "           1       0.42      0.19      0.26      5536\n",
      "           2       0.64      0.91      0.75     20768\n",
      "           3       0.47      0.07      0.12      2599\n",
      "\n",
      "    accuracy                           0.61     34175\n",
      "   macro avg       0.48      0.32      0.33     34175\n",
      "weighted avg       0.55      0.61      0.54     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5376627277328271"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb=meboost.AdaBoostClassifier()\n",
    "mb.fit(Xt, yt)\n",
    "y_pred = mb.predict(Xv)\n",
    "accuracy = accuracy_score(yv, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.42%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.44      0.44      5272\n",
      "           1       0.32      0.33      0.33      5536\n",
      "           2       0.70      0.68      0.69     20768\n",
      "           3       0.22      0.24      0.23      2599\n",
      "\n",
      "    accuracy                           0.55     34175\n",
      "   macro avg       0.42      0.42      0.42     34175\n",
      "weighted avg       0.56      0.55      0.56     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5567942019257687"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb=meboost.DecisionTreeClassifier()\n",
    "mb.fit(Xt, yt)\n",
    "y_pred = mb.predict(Xv)\n",
    "accuracy = accuracy_score(yv, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepak/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.02%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.42      0.46      5272\n",
      "           1       0.44      0.34      0.38      5536\n",
      "           2       0.70      0.82      0.75     20768\n",
      "           3       0.37      0.16      0.22      2599\n",
      "\n",
      "    accuracy                           0.63     34175\n",
      "   macro avg       0.50      0.44      0.46     34175\n",
      "weighted avg       0.60      0.63      0.61     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6083989633808263"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb=meboost.RandomForestClassifier()\n",
    "mb.fit(Xt, yt)\n",
    "y_pred = mb.predict(Xv)\n",
    "accuracy = accuracy_score(yv, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepak/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.61%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.43      0.47      5272\n",
      "           1       0.43      0.35      0.39      5536\n",
      "           2       0.70      0.80      0.75     20768\n",
      "           3       0.35      0.19      0.24      2599\n",
      "\n",
      "    accuracy                           0.63     34175\n",
      "   macro avg       0.50      0.44      0.46     34175\n",
      "weighted avg       0.60      0.63      0.61     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6085125878040818"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb=meboost.ExtraTreesClassifier()\n",
    "mb.fit(Xt, yt)\n",
    "y_pred = mb.predict(Xv)\n",
    "accuracy = accuracy_score(yv, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUS\n",
    "X_full = final_df.copy()\n",
    "y_train = X_full['final_result']\n",
    "X_maj = X_full[X_full.final_result==2]\n",
    "X_min = X_full[X_full.final_result!=2]\n",
    "X_maj_rus = resample(X_maj,replace=False,random_state=44)\n",
    "X_rus = pd.concat([X_maj_rus, X_min])\n",
    "X_train_rus = X_rus.drop(['final_result'], axis=1)\n",
    "y_train_rus = X_rus.final_result\n",
    "# y_rus = adaboost(X_train_rus, X_test, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 104129, 3: 13023, 1: 28002, 0: 25720})"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(y_train_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(Xt, yt)\n",
    "# y_smote = adaboost(X_train_sm, X_test, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 83361, 1: 83361, 0: 83361, 3: 83361})"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(y_train_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# fit model no training data\n",
    "model_rus_sm = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 83361, 1: 83361, 0: 83361, 3: 83361})"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(list(y_train_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='multi:softprob',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rus_sm.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='multi:softprob',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_normal = XGBClassifier()\n",
    "model_normal.fit(Xt, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='multi:softprob',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rus = XGBClassifier()\n",
    "model_rus.fit(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='multi:softprob',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sm, y_train_sm = sm.fit_sample(Xt, yt)\n",
    "model_sm = XGBClassifier()\n",
    "model_sm.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.99%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.67      0.47      5272\n",
      "           1       0.35      0.37      0.36      5536\n",
      "           2       0.74      0.48      0.58     20768\n",
      "           3       0.25      0.49      0.33      2599\n",
      "\n",
      "    accuracy                           0.49     34175\n",
      "   macro avg       0.42      0.50      0.43     34175\n",
      "weighted avg       0.58      0.49      0.51     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5076069479887825"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_rus_sm.predict(Xv.values)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(yv.values, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.63%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.62      0.46      5272\n",
      "           1       0.36      0.32      0.34      5536\n",
      "           2       0.72      0.55      0.62     20768\n",
      "           3       0.26      0.45      0.33      2599\n",
      "\n",
      "    accuracy                           0.52     34175\n",
      "   macro avg       0.43      0.49      0.44     34175\n",
      "weighted avg       0.57      0.52      0.53     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5303168507923985"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_sm.predict(Xv.values)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(yv.values, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.56%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.14      0.23      5272\n",
      "           1       0.49      0.15      0.23      5536\n",
      "           2       0.64      0.96      0.77     20768\n",
      "           3       0.56      0.09      0.15      2599\n",
      "\n",
      "    accuracy                           0.64     34175\n",
      "   macro avg       0.58      0.34      0.35     34175\n",
      "weighted avg       0.61      0.64      0.55     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5536936580943164"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_rus.predict(Xv)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(yv.values, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.41%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.14      0.23      5272\n",
      "           1       0.48      0.15      0.23      5536\n",
      "           2       0.64      0.96      0.77     20768\n",
      "           3       0.53      0.09      0.15      2599\n",
      "\n",
      "    accuracy                           0.63     34175\n",
      "   macro avg       0.58      0.33      0.34     34175\n",
      "weighted avg       0.61      0.63      0.55     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5508060299177262"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_normal.predict(Xv)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(yv.values, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save model to file\n",
    "pickle.dump(model, open(\"model_xbg_oulad_rus_sm.pickle.dat\", \"wb\"))\n",
    "model = pickle.load(open(\"model_xbg_oulad_rus.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.31      0.38      5272\n",
      "           1       0.44      0.28      0.34      5536\n",
      "           2       0.67      0.87      0.76     20768\n",
      "           3       0.52      0.11      0.17      2599\n",
      "\n",
      "    accuracy                           0.63     34175\n",
      "   macro avg       0.53      0.39      0.41     34175\n",
      "weighted avg       0.60      0.63      0.59     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6304901243599123"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.31      0.38      5272\n",
      "           1       0.44      0.28      0.34      5536\n",
      "           2       0.67      0.87      0.76     20768\n",
      "           3       0.52      0.11      0.17      2599\n",
      "\n",
      "    accuracy                           0.63     34175\n",
      "   macro avg       0.53      0.39      0.41     34175\n",
      "weighted avg       0.60      0.63      0.59     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5884825227369722"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open(\"model_xbg_oulad.pickle.dat\", \"rb\"))\n",
    "y_pred = model.predict(Xv)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, balanced_accuracy_score, f1_score, classification_report\n",
    "accuracy = accuracy_score(yv.values, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.99%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.67      0.47      5272\n",
      "           1       0.35      0.37      0.36      5536\n",
      "           2       0.74      0.48      0.58     20768\n",
      "           3       0.25      0.49      0.33      2599\n",
      "\n",
      "    accuracy                           0.49     34175\n",
      "   macro avg       0.42      0.50      0.43     34175\n",
      "weighted avg       0.58      0.49      0.51     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5076069479887825"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open(\"model_xbg_oulad_rus_sm.pickle.dat\", \"rb\"))\n",
    "y_pred = model.predict(Xv.values)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, balanced_accuracy_score, f1_score, classification_report\n",
    "accuracy = accuracy_score(yv.values, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.31      0.38      5272\n",
      "           1       0.44      0.28      0.34      5536\n",
      "           2       0.67      0.87      0.76     20768\n",
      "           3       0.52      0.11      0.17      2599\n",
      "\n",
      "    accuracy                           0.63     34175\n",
      "   macro avg       0.53      0.39      0.41     34175\n",
      "weighted avg       0.60      0.63      0.59     34175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5884825227369722"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open(\"model_xbg_oulad_rus.pickle.dat\", \"rb\"))\n",
    "y_pred = model.predict(Xv)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, balanced_accuracy_score, f1_score, classification_report\n",
    "accuracy = accuracy_score(yv.values, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(yv, y_pred))\n",
    "f1_score(yv, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"model_xbg_oulad_rus_sm.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.99%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(Xv.values)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, balanced_accuracy_score, f1_score, classification_report\n",
    "accuracy = accuracy_score(yv.values, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "f1_score(yv, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5020545508359444"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(yv, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.67      0.47      5272\n",
      "           1       0.35      0.37      0.36      5536\n",
      "           2       0.74      0.48      0.58     20768\n",
      "           3       0.25      0.49      0.33      2599\n",
      "\n",
      "    accuracy                           0.49     34175\n",
      "   macro avg       0.42      0.50      0.43     34175\n",
      "weighted avg       0.58      0.49      0.51     34175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yv, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4342158397427065"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(yv, y_pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
